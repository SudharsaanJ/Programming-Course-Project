---
title: "ST2195 Coursework question 5"
author: "UOL Student Number 200549079"
date: "2/25/2022"
output: html_document
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

```

**Setting up working directory**

```{r setting & checking working directory}

setwd("C:/Users/Sudharsaan/OneDrive/SIM/Year 2/ST2195 Programming for Data Science/Coursework/Essential Harvard Dataverse Files")
getwd()

```

**Installing & Loading packages in R**

```{r Installing & Loading packages in R}

#install.packages("dplyr")
library(dplyr)

#install.packages("DBI")
library(DBI)

#install.packages("skimr")
library(skimr)

#install.packages("ggplot2")
library(ggplot2)

#install.packages("ggthemes")
library(ggthemes)

#install.packages("mlr3")
library(mlr3)

#install.packages("mlr3learners")
library(mlr3learners)

#install.packages("mlr3pipelines")
library(mlr3pipelines)

#install.packages("mlr3tuning")
library(mlr3tuning)

#install.packages("paradox")
library(paradox)

#install.packages("mlr3viz")
library(mlr3viz)

#install.packages("glmnet")
library(glmnet)

#install.packages("future")
library(future)

```

**Creating connection with airlinemain_r.db (already created)**

```{r Connecting with database}

conn <- dbConnect(RSQLite::SQLite(), "airlinemain_r.db")

```





**Performing Query on question 5 of coursework**

Question 5 asks to use the available variables construct a model that predicts delays. Delays in the context of our model refers to the arrival delay. Thus, we will be building a model that predicts **arrival delay (target/response variable)** of flights that is based on data collected in 2006 & 2007. The **features** that will be chosen for bulding the model are:
- CarrierDelay, WeatherDelay, NASDelay, SecurityDelay, LateAircraftDelay, DepDelay, & AgeofPlane.

Take note that we remove cancelled & diverted flights from the dataset as rows of cancelled & diverted flights contain many null values in its features. This is due to the nature of cancelled & diverted flights. Furthermore, we aim to use age of plane as one of the features. In question 2, we queried for planes between ages 0 & 61 given the years the planes were manufactured & the latest year of flight in our dataset. Thus, when querying for age of planes as one of the features in our model, we once again restrict planes whose ages are below 0 & above 61 as they are considered erroneous data given the context of our analysis.

```{r Querying information regarding features & response variable required to build the model}

q5_dataset <- dbSendQuery(conn,
                          "SELECT CarrierDelay, WeatherDelay, NASDelay, SecurityDelay, LateAircraftDelay, DepDelay, (ontime.Year - planes.year) AS AgeofPlane, ArrDelay
                          FROM ontime JOIN planes ON ontime.TailNum = planes.tailnum
                          WHERE Cancelled = 0 AND Diverted = 0 AND (ontime.Year - planes.year) > -1 AND (ontime.Year - planes.year) < 62")

q5_dataset <- dbFetch(q5_dataset)
q5_dataset <- as.data.frame(q5_dataset)
q5_dataset

```



**Creating Models to predict Arrival Delay**

We will be creating **3 different models** to predict arrival delay & assessing which model is the best using various **metrics**. The models to be created are:
1. Linear Regression Model
2. Lasso Regression Model
3. Ridge Regression Model

```{r Specifying task for regression}

# TaskRegr is used to perform regression since our target variable, "ArrDelay", is a continuous variable. 
task <- TaskRegr$new(id = 'arrival_delay', backend = q5_dataset, target = 'ArrDelay')

task$nrow # Shows number of rows in our dataset. 
task$ncol # Shows number of columns in our dataset.

print(task)

print(task$feature_names) # Shows the name of the features in our dataset. 
print(task$target_names) # Shows the name of the target/response variable in our dataset. 
print(task$missings()) # Shows that we have no missing values in our dataset. 
# print(task$data()) # Shows the dataset that the task is working with for the regression. 

```

```{r Setting up measure used to evaluate model performance}

measure <- msr("regr.mse")

```



**Linear Regression Model**

```{r Creating Linear Regression Model on Arrival Delays}

# Choosing linear regression as one of the learners for this regression task.
learner_lm <- lrn("regr.lm")

# Performing scaling & handling missing values for this linear regression machine learning model. 
gr_lm <- po("scale") %>>% po("imputemean") %>>%
                          po(learner_lm)

# Combined learner (Combines the pre-processing stage & machine learning model stage)
glrn_lm <- GraphLearner$new(gr_lm)

# For standard linear regression, there isn't any hyperparameters. Thus, we will not be tuning hyperparameters for our linear regression model.  


```

```{r Training & Prediction of Linear Regression Model}

future::plan()

# Ensures reproducibility of result
set.seed(1)

# Splitting dataset into train & test sets.
train_set <- sample(task$nrow, 0.7 * task$nrow)
test_set <- setdiff(seq_len(task$nrow), train_set)

# Training of linear regression model
glrn_lm$train(task, row_ids = train_set)

# Prediction of arrival delay values using linear regression model & attaining Mean Squared Error value to evaluate performance of model.
prediction_lm <- glrn_lm$predict(task, row_ids = test_set)$score()
prediction_lm

```

```{r Plotting regression line for predicted & actual values in test set for Linear Regression Model}

predict_values_lm <- glrn_lm$predict(task, row_ids = test_set)
predict_values_lm <- as.list(predict_values_lm)

predict_values_lm_response <- as.data.frame(predict_values_lm$response)
head(predict_values_lm_response)
predict_values_lm_truth <- as.data.frame(predict_values_lm$truth)
head(predict_values_lm_truth)

predict_values_lm <- cbind(predict_values_lm_truth, predict_values_lm_response)
names(predict_values_lm) <- c("Actual_ArrDelay", "Predicted_ArrDelay") 

min(predict_values_lm$Actual_ArrDelay)
min(predict_values_lm$Predicted_ArrDelay)

max(predict_values_lm$Actual_ArrDelay)
max(predict_values_lm$Predicted_ArrDelay)


q5_lm_scatterplot <- ggplot(predict_values_lm) + geom_point(aes(x = predict_values_lm[,2], y = predict_values_lm[,1]), colour = "darkred") + geom_smooth(aes(x = predict_values_lm[,2], y = predict_values_lm[,1]), se = FALSE, method = lm, colour = "red") + labs(title = "Figure 5.1: Linear Regression Model on Arrival Delay", x = "Predicted Arrival Delays", y = "Actual Arrival Delays") + theme_economist() + xlim(-600,2800) + ylim(-600, 2800)

q5_lm_scatterplot

```



**Lasso Regression Model**

```{r Tuning Hyperparameters for Lasso Regression Model}

# Ensuring reproducible results
set.seed(1)

# Setting up tune environment for tuning of hyperparameters. We specify the range that the parameter "lambda" can take for the lasso regression is between 0.001 & 2. 
tune_lambda <- ParamSet$new(list(
  ParamDbl$new("regr.glmnet.lambda", lower = 0.001, upper = 2)
))

# Specifying how the search will be performed (grid search refers to a systematic way of searching). There exist other forms of searching such as "random search".
tuner <- tnr("grid_search")

# Specifying the number of times the procedure will be repeated to be 3 times. Evaluation is defined as one resampling of a parameter value. 
terminator <- trm("evals", n_evals = 3)

```

```{r Creating Lasso Regression Model on Arrival Delays}

# Choosing lasso regression as one of the learners for this regression task.
learner_lasso <- lrn("regr.glmnet")

# We set the parameter alpha's value to 1 to specify the use of lasso regression. 
learner_lasso$param_set$values <- list(alpha = 1)

# Performing scaling & handling missing values for this lasso regression machine learning model. 
gr_lasso <- po("scale") %>>% po("imputemean") %>>% 
                             po(learner_lasso)

# Combined learner (Combines the pre-processing stage & machine learning model stage)
glrn_lasso <- GraphLearner$new(gr_lasso)

# Putting the tuning of hyperparameter with the combined learner to create a new learner
at_lasso <- AutoTuner$new(
  learner = glrn_lasso,
  resampling = rsmp("cv", folds = 3),
  measure = measure,
  search_space = tune_lambda,
  terminator = terminator,
  tuner = tuner
)

```

```{r Training & Prediction of Lasso Regression Model}

future::plan()

# Ensures reproducibility of result
set.seed(1)

# Training of lasso regression model
at_lasso$train(task, row_ids = train_set)

# Prediction of arrival delay values using lasso regression model & attaining Mean Squared Error (MSE) value to evaluate performance of model.
prediction_lasso <- at_lasso$predict(task, row_ids = test_set)$score()
prediction_lasso

```

```{r Plotting regression line for predicted & actual values in test set for Lasso Regression Model}

# Converting object of "environment" type to a data frame by changing it into a list before changing parts of the list into a dataframe. Take note that using as.data.frame() results in errors. 
predict_values_lasso <- at_lasso$predict(task, row_ids = test_set)
predict_values_lasso <- as.list(predict_values_lasso)

predict_values_lasso_response <- as.data.frame(predict_values_lasso$response)
head(predict_values_lasso_response)
predict_values_lasso_truth <- as.data.frame(predict_values_lasso$truth)
head(predict_values_lasso_truth)

predict_values_lasso <- cbind(predict_values_lasso_truth, predict_values_lasso_response)
names(predict_values_lasso) <- c("Actual_ArrDelay", "Predicted_ArrDelay") 

min(predict_values_lasso$Predicted_ArrDelay)
max(predict_values_lasso$Predicted_ArrDelay)


q5_lasso_scatterplot <- ggplot(predict_values_lasso) + geom_point(aes(x = predict_values_lasso[,2], y = predict_values_lasso[,1]), colour = "darkgreen") + geom_smooth(aes(x = predict_values_lasso[,2], y = predict_values_lasso[,1]), se = FALSE, method = lm, colour = "green") + labs(title = "Figure 5.2: Lasso Regression Model on Arrival Delay", x = "Predicted Arrival Delays", y = "Actual Arrival Delays") + theme_economist() + xlim(-600,2800) + ylim(-600, 2800)

q5_lasso_scatterplot

```



**Ridge Regression Model**

```{r Tuning Hyperparameters for Ridge Regression Model}

# Tuning of hyperparameters for the ridge regression model is identical to the tuning of hyperparameters for lasso regression model. You may choose to skip this part as the declaration of variables is unchanged. We redeclare the variables to show that hyperparameter tuning is taking place for the ridge regression model.  

# Ensuring reproducible results
set.seed(1)

# Setting up tune environment for tuning of hyperparameters. We specify the range that the parameter "lambda" can take for the ridge regression is between 0.001 & 2. 
tune_lambda <- ParamSet$new(list(
  ParamDbl$new("regr.glmnet.lambda", lower = 0.001, upper = 2)
))

# Specifying how the search will be performed (grid search refers to a systematic way of searching). There exist other forms of searching such as "random search".
tuner <- tnr("grid_search")

# Specifying the number of times the procedure will be repeated to be 3 times. Evaluation is defined as one resampling of a parameter value. 
terminator <- trm("evals", n_evals = 3)

```

```{r Creating Ridge Regression Model on Arrival Delays}

# Choosing ridge regression as one of the learners for this regression task.
learner_ridge <- lrn("regr.glmnet")

# We set the parameter alpha's value to 0 to specify the use of ridge regression. 
learner_ridge$param_set$values <- list(alpha = 0)

# Performing scaling & handling missing values for this ridge regression machine learning model. 
gr_ridge <- po("scale") %>>% po("imputemean") %>>% 
                             po(learner_ridge)

# Combined learner (Combines the pre-processing stage & machine learning model stage)
glrn_ridge <- GraphLearner$new(gr_ridge)

# Putting the tuning of hyperparameter with the combined learner to create a new learner
at_ridge <- AutoTuner$new(
  learner = glrn_ridge,
  resampling = rsmp("cv", folds = 3),
  measure = measure,
  search_space = tune_lambda,
  terminator = terminator,
  tuner = tuner
)

```

```{r Training & Prediction of Ridge Regression Model}

future::plan()

# Ensures reproducibility of result
set.seed(1)

# Training of ridge regression model
at_ridge$train(task, row_ids = train_set)

# Prediction of arrival delay values using ridge regression model & attaining Mean Squared Error (MSE) value to evaluate performance of model.
prediction_ridge <- at_ridge$predict(task, row_ids = test_set)$score()
prediction_ridge

```

```{r Plotting regression line for predicted & actual values in test set for Ridge Regression Model}

# Converting object of "environment" type to a data frame by changing it into a list before changing parts of the list into a dataframe. Take note that using as.data.frame() results in errors. 
predict_values_ridge <- at_ridge$predict(task, row_ids = test_set)
predict_values_ridge <- as.list(predict_values_ridge)

predict_values_ridge_response <- as.data.frame(predict_values_ridge$response)
head(predict_values_ridge_response)
predict_values_ridge_truth <- as.data.frame(predict_values_ridge$truth)
head(predict_values_ridge_truth)

predict_values_ridge <- cbind(predict_values_ridge_truth, predict_values_ridge_response)
names(predict_values_ridge) <- c("Actual_ArrDelay", "Predicted_ArrDelay") 

min(predict_values_ridge$Predicted_ArrDelay)
max(predict_values_ridge$Predicted_ArrDelay)


q5_ridge_scatterplot <- ggplot(predict_values_ridge) + geom_point(aes(x = predict_values_ridge[,2], y = predict_values_ridge[,1]), colour = "darkblue") + geom_smooth(aes(x = predict_values_ridge[,2], y = predict_values_ridge[,1]), se = FALSE, method = lm, colour = "blue") + labs(title = "Figure 5.3: Ridge Regression Model on Arrival Delay", x = "Predicted Arrival Delays", y = "Actual Arrival Delays") + theme_economist() + xlim(-600,2800) + ylim(-600, 2800)

q5_ridge_scatterplot

```

```{r Comparing metric scores for all models created}

metrics <- list(prediction_lm, prediction_lasso, prediction_ridge)
metrics <- as.data.frame(metrics)
names(metrics) <- c("Linear", "Lasso", "Ridge")
rownames(metrics) <- c("Mean Squared Error")
metrics

```

**Observations from table above**

From the table above, we can observe that the Linear Regression model has the lowest Mean Squared Error (MSE) compared to the other models, Lasso & Ridge Regression.

**Conclusion**

Thus, **Linear Regression model** is the **best model** out of the 3 models created in predicting arrival delays. 

```{r Closing connection to database}

dbDisconnect(conn)

```















